{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Deeplearning-notes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXryh40iknyRePnpNrg+7w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayan-roy/Deep-learning-using-Boston-Housing-dataset/blob/master/Deeplearning_housing_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEsxkeqokb90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2iI75gglbNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "843a40b4-e7d6-4315-bba3-de34098bcfed"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSFOCaiHYDhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLnho3G2IjYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bostondata = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/Datasets/master/Boston_Housing/Training_set_boston.csv\" )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKEoHKY7IoDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "52988350-3662-4315-f755-90b4d6fd533f"
      },
      "source": [
        "bostondata.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>MEDV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15.02340</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6140</td>\n",
              "      <td>5.304</td>\n",
              "      <td>97.3</td>\n",
              "      <td>2.1007</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>349.48</td>\n",
              "      <td>24.91</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.62739</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5380</td>\n",
              "      <td>5.834</td>\n",
              "      <td>56.5</td>\n",
              "      <td>4.4986</td>\n",
              "      <td>4.0</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>395.62</td>\n",
              "      <td>8.47</td>\n",
              "      <td>19.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.03466</td>\n",
              "      <td>35.0</td>\n",
              "      <td>6.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4379</td>\n",
              "      <td>6.031</td>\n",
              "      <td>23.3</td>\n",
              "      <td>6.6407</td>\n",
              "      <td>1.0</td>\n",
              "      <td>304.0</td>\n",
              "      <td>16.9</td>\n",
              "      <td>362.25</td>\n",
              "      <td>7.83</td>\n",
              "      <td>19.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.05042</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6140</td>\n",
              "      <td>6.103</td>\n",
              "      <td>85.1</td>\n",
              "      <td>2.0218</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>2.52</td>\n",
              "      <td>23.29</td>\n",
              "      <td>13.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.72580</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5380</td>\n",
              "      <td>5.727</td>\n",
              "      <td>69.5</td>\n",
              "      <td>3.7965</td>\n",
              "      <td>4.0</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>390.95</td>\n",
              "      <td>11.28</td>\n",
              "      <td>18.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       CRIM    ZN  INDUS  CHAS     NOX  ...    TAX  PTRATIO       B  LSTAT  MEDV\n",
              "0  15.02340   0.0  18.10   0.0  0.6140  ...  666.0     20.2  349.48  24.91  12.0\n",
              "1   0.62739   0.0   8.14   0.0  0.5380  ...  307.0     21.0  395.62   8.47  19.9\n",
              "2   0.03466  35.0   6.06   0.0  0.4379  ...  304.0     16.9  362.25   7.83  19.4\n",
              "3   7.05042   0.0  18.10   0.0  0.6140  ...  666.0     20.2    2.52  23.29  13.4\n",
              "4   0.72580   0.0   8.14   0.0  0.5380  ...  307.0     21.0  390.95  11.28  18.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-g0gZDlIvy4",
        "colab_type": "text"
      },
      "source": [
        "Before we do any analysis, we always want to separate the data as input and output variable.\n",
        "\n",
        "Our target column is MEDV\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaH64SJsI964",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We select 1 on the bottom because we are dropping in a column and not row\n",
        "X = bostondata.drop(\"MEDV\", axis = 1) \n",
        "y = bostondata.MEDV \n",
        "# Or you can do y = boston_data[\"MEDV\"] "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlWXDR42JQJp",
        "colab_type": "text"
      },
      "source": [
        "## Splitting the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ed2N2zNJTHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# We are assigning 20% to the test data and 80% to the train data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# random_state = 42: this will fix the split i.e. there will be same split for each time you run the code"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwctJDgAJrmp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4048afa-6834-43cd-a041-63c1a997c19f"
      },
      "source": [
        "# Finding the number of feature\n",
        "# Note 1 stands for no of column, while 0 stands for no of rows\n",
        "n_features = X.shape[1]\n",
        "print(n_features)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPe0uTbVMj8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will be using Sequential API\n",
        "#It is referred to as “sequential” because it involves defining a Sequential class and \n",
        "#adding layers to the model one by one in a linear manner, from input to output."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mzYfsPANF6E",
        "colab_type": "text"
      },
      "source": [
        "The example below defines a Sequential MLP model that accepts one input (i.e. 'YearsExperience'), has one hidden layer with 1 node and then an output layer with one node to predict a numerical value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAbN-CoqNIeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential    # import Sequential from tensorflow.keras\n",
        "from tensorflow.keras.layers import Dense  # import Dense from tensorflow.keras.layers\n",
        "from numpy.random import seed     # seed helps you to fix the randomness in the neural network.  \n",
        "import tensorflow"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8cxm0sLNek7",
        "colab_type": "text"
      },
      "source": [
        "Note that the visible layer of the network is defined by the “input_shape” argument on the first hidden layer. That means in the below example, the model expects the input for one sample to be a vector of n_features (our case its 13)\n",
        "\n",
        "\n",
        "The sequential API is easy to use because you keep calling model.add() until you have added all of your layers\n",
        "\n",
        "\n",
        "The activation function we have chosen is ReLU, which stands for rectified linear unit. Activation function decides, whether a neuron should be activated or not\n",
        "\n",
        "\n",
        "ReLU is defined mathematically as F(x) = max(0,x). In other words, the output is x, if x is greater than 0, and the output is 0 if x is 0 or negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIvWNpnnNdfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "# We added 3 layers\n",
        "model.add(Dense(10, activation='relu', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMvXGIJJOgNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "21ccd4b4-ccaf-4e9a-b21a-23a6ee222f2e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 10)                140       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 88        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 237\n",
            "Trainable params: 237\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5prgx5-O5Ec",
        "colab_type": "text"
      },
      "source": [
        "An Optimizer helps optimize a cost function\n",
        "\n",
        "## Compiling the Model\n",
        "\n",
        "\n",
        "Compiling the model requires that you first select a loss function that you want to optimize, such as mean squared error (lin regression) or cross-entropy (used for binary classification).\n",
        "\n",
        "It also requires that you select an algorithm to perform the optimization procedure. We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation.\n",
        "\n",
        "It may also require that you select any performance metrics to keep track of during the model training process. The loss function used here is mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k3GGVtoO_Yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import RMSprop optimizer\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "optimizer = RMSprop(0.01)    # 0.01 is the learning rate"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLSgv9eaPX9C",
        "colab_type": "text"
      },
      "source": [
        "You may wonder why learning rate of 0.01? \n",
        "Usually its between 1 and 10^(-6) but you it can be determined using trial and error\n",
        "\n",
        "Tradtionally its 0.1, or 0.01 or 0.03\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJeBE5mLPqA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model\n",
        "# We see the optimizer was a RmSprop which is also root mean square propagation"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVk5WAlCQF0T",
        "colab_type": "text"
      },
      "source": [
        "## Fitting the model\n",
        "Fitting the model requires that you first select the training configuration, such as the number of epochs (loops through the training dataset) and the batch size (number of samples in an epoch used to estimate model error).\n",
        "\n",
        "Training applies the chosen optimization algorithm to minimize the chosen loss function and updates the model using the backpropagation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Fitting the model is the slow part of the whole process and can take seconds to hours to days, depending on the complexity of the model, the hardware you’re using, and the size of the training dataset.\n",
        "\n",
        "While fitting the model, a progress bar will summarize the status of each epoch and the overall training process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtaOEqYySis6",
        "colab_type": "text"
      },
      "source": [
        "**Seed Everything**\n",
        "We want to seed everything in order to replicate or have consistent randomization in code. The weights of paramters are usually randomized.\n",
        "When we seed, we can reproduce these results.\n",
        "\n",
        "**So say we said 42, if we say 80  next time, and then again 42, both times when 42 is used, we get the same results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmPbUkRuTj68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfd03057-a91c-4fd7-dc23-bb09f3cccf9b"
      },
      "source": [
        "X_train.shape #  => (323,13)\n",
        "X_train.shape[0] # => 323\n",
        "X_train.shape[1] # => 13"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhrLM6JnQcD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a93c6d1a-9d0d-4272-d1ec-7a98957b2862"
      },
      "source": [
        "seed_value = 42\n",
        "seed(seed_value)        # If you build the model with given parameters, \n",
        "#set_random_seed will help you produce the same result on multiple execution\n",
        "\n",
        "\n",
        "# Recommended by Keras --------------------------------------------------------\n",
        "# 1. Set 'PYTHONHASHSEED' environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set 'python' built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set 'numpy' pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "# Recommended by Keras -------------------------------------------------------\n",
        "\n",
        "\n",
        "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
        "tensorflow.random.set_seed(seed_value) \n",
        "model.fit(X_train, y_train, epochs=10, batch_size=30, verbose = 1)\n",
        "\n",
        "# By now the model has learned the coefficient m, and c of mx+c"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 325.7995\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 87.7003\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 102.2011\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 97.3594\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 73.5658\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 100.7587\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 72.4126\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 88.2007\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 88.9650\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 87.7681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff9d378dba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK5l8AcEQ43r",
        "colab_type": "text"
      },
      "source": [
        "What is verbose?\n",
        "\n",
        "By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch.\n",
        "\n",
        "verbose=0 will show you nothing (silent)\n",
        "\n",
        "verbose=1 will show you an animated progress bar like this: [======]\n",
        "progres_bar\n",
        "\n",
        "verbose=2 will just mention the number of epoch like this: Epoch 1/10\n",
        "\n",
        "verbose = 2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WL7kCX2Rb8G",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the Model\n",
        "\n",
        "Evaluating the model requires that you first choose small subset of a dataset used to evaluate the model. This should be data not used in the training process i.e. the X_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn__XKrjRjy6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "853b36be-073d-4560-8194-be322beb5202"
      },
      "source": [
        "model.evaluate(X_test, y_test) #evaluate on the 20% of the test based on the model coefficent that it was trained"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 2ms/step - loss: 50.1649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50.16490936279297"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZRysiKuRtO2",
        "colab_type": "text"
      },
      "source": [
        "The mean squared error we got here is 50.16. Now, what does it mean?\n",
        "\n",
        "When you subtract the predicted values (of X_test data) from the acutal value (of X_test data), then square it and sum all the squares, and finally take a mean (i.e. average) of it, the result you will get is 50.16 in this case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN_2-2XfR1qM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(X_test) #Gives the predicted Y values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR_mwvJCUt3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting the loss curve:\n",
        "\n",
        "def plotting_the_loss_curve(epochs, rmse):\n",
        "  \"Returns the curve of loss vs epoch\"\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Root Mean Squared Error\")\n",
        "  plt.plot(epochs, rmse,label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzrj932e3W_f",
        "colab_type": "text"
      },
      "source": [
        "## Hyper Parameter tuning\n",
        "We will be testing different rates to find an optimal option\n",
        "\n",
        "**Learning Rate**\n",
        "\n",
        "A scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step.\n",
        "\n",
        "Learning rate is a key hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DQvxrJ23SXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "53bc0157-69af-455e-9ada-ef826769d427"
      },
      "source": [
        "# Model 1\n",
        "# Changing alpha from 0.01 to 0.1\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "optimizer = RMSprop(0.1)    # 0.1 is the learning rate\n",
        "model.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model\n",
        "\n",
        "# fit the model \n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=30, verbose = 1)\n",
        "\n",
        "# evaluate the model\n",
        "print('The MSE value is: ', model.evaluate(X_test, y_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 9724.5732\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 582.0069\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 555.8216\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 521.9396\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 483.8689\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 444.7491\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 406.9054\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 371.2612\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 337.6587\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 306.2268\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 271.3606\n",
            "The MSE value is:  271.3605651855469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UagNZztD4dbo",
        "colab_type": "text"
      },
      "source": [
        "We see that the loss is 306.2268, which is worst that we had. Note you look at the last loss and not the MSE value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Puuilhdo43jK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "7a45f2bd-2981-432b-fcfd-cd7a4451cd44"
      },
      "source": [
        "# TO observe the deviation\n",
        "plotting_the_loss_curve(history.epoch, history.history[\"loss\"])\n",
        "# You need to use history function for some reason, don't know why"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJQ9AOMM6U3C",
        "colab_type": "text"
      },
      "source": [
        "**As the number of epochs increases, the model learns better and RMSE starts ro decrease**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPj2yTgbZt9M",
        "colab_type": "text"
      },
      "source": [
        "We will be testing different learning rate and how it works\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5stuqoiZ3Qv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a0b285de-0d8e-4364-a876-3b7dcd339a19"
      },
      "source": [
        "learning_rate = 0.03          \n",
        "epochs = 10\n",
        "optimizer = RMSprop(learning_rate)\n",
        "model.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=30)     # fit the model\n",
        "model.evaluate(X_test, y_test)       # Evaluate the model"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 281.2031\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 269.7977\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 260.7584\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 252.4206\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 244.3937\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 236.4837\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 228.8299\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 221.4789\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 214.3056\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 0s 1ms/step - loss: 207.3204\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 184.8917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "184.89169311523438"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ryHUR0acco",
        "colab_type": "text"
      },
      "source": [
        "We see that as we decreased the learning rate from 0.01 to 0.03 the loss decreased to 184.8917"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx9e4za7arCl",
        "colab_type": "text"
      },
      "source": [
        "Now, we are going to test with **epochs**\n",
        "In the code below, Epoch will be increased from 10 to 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ashvq-vWau3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.03          \n",
        "epochs = 100\n",
        "optimizer = RMSprop(learning_rate)\n",
        "model.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=30)     # fit the model\n",
        "model.evaluate(X_test, y_test)       # Evaluate the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4J-oFhLdI4v",
        "colab_type": "text"
      },
      "source": [
        "It can be seen as we increased the epoch while keep the **learning rate** same/constant, the loss decreased to 71.\n",
        "\n",
        "Note the downside is that the model might overfit too much by learning the pattern, which is not good either"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvpTCeibdqPn",
        "colab_type": "text"
      },
      "source": [
        "Now we will test with **Batch Size** which is basically the number of batches in a batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtDDVR_WdeMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model 2\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "optimizer = RMSprop(0.01)    # 0.1 is the learning rate\n",
        "model.compile(loss='mean_squared_error',optimizer=optimizer)   \n",
        "\n",
        "# fit the model \n",
        "model.fit(X_train, y_train, epochs=10, batch_size=40, verbose = 1) # Batch size increased from 30 to 40\n",
        "\n",
        "# evaluate the model\n",
        "print('The MSE value is: ', model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF9dOgyseg29",
        "colab_type": "text"
      },
      "source": [
        "We see that increasing the batch size made the model better as lowest loss was 122 as compared to model 1 (right below **learning rate**), which was 306. \n",
        "\n",
        "What we learn is that we need to be testing in order to see what fits the model. The law of large numbers may not work all the time, where higher the n the better it is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNPF43gSfFHH",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Most machine learning problems require a lot of hyperparameter tuning. \n",
        "\n",
        "Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly. We must experiment to find the best set of hyperparameters"
      ]
    }
  ]
}
